"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FASTAPI ML MICROSERVICE - MAIN APPLICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Any
import numpy as np
import joblib
import logging
from datetime import datetime
import os
from pathlib import Path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create logs directory if it doesn't exist
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# Configure logging with file and console handlers
logging.basicConfig(
    level=logging.DEBUG,  # Log everything
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        # Console handler
        logging.StreamHandler(),
        # File handler for all logs
        logging.FileHandler(log_dir / 'ml-service.log'),
        # File handler for errors only
        logging.FileHandler(log_dir / 'error.log', mode='a'),
    ]
)

# Set error handler to only log errors
error_handler = logging.getLogger().handlers[2]
error_handler.setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
logger.info("ML Service logging configured - logging everything to files")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL_PATH = os.getenv("MODEL_PATH", "./models/rl_agent_ppo.pkl")
MODEL_TYPE = os.getenv("MODEL_TYPE", "PPO")
MODEL_VERSION = os.getenv("MODEL_VERSION", "1.0.0")
CORS_ORIGINS = os.getenv("CORS_ORIGINS", "http://localhost:5000").split(",")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYDANTIC MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Position(BaseModel):
    """Position within a portfolio"""
    symbol: str
    quantity: float
    price: float
    delta: Optional[float] = None
    gamma: Optional[float] = None
    vega: Optional[float] = None
    theta: Optional[float] = None

class PortfolioData(BaseModel):
    """Portfolio data for ML prediction"""
    totalValue: float = Field(..., gt=0, description="Total portfolio value")
    positions: List[Position] = Field(..., min_items=1)
    historicalReturns: Optional[List[float]] = None

class MLPredictionRequest(BaseModel):
    """Request model for risk prediction"""
    portfolioId: str
    portfolioData: PortfolioData

class MLPredictionResponse(BaseModel):
    """Response model for risk prediction"""
    riskScore: int = Field(..., ge=0, le=100)
    volatility: float
    var95: float
    var99: float
    sharpeRatio: float
    recommendation: str
    confidence: float = Field(..., ge=0, le=1)
    timestamp: str

class HedgingRecommendation(BaseModel):
    """Hedging recommendation response"""
    action: str
    contracts: int
    strategy: str
    expectedReduction: float

class ModelInfo(BaseModel):
    """ML model information"""
    name: str
    version: str
    trained_at: str
    performance_metrics: Dict[str, float]

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    timestamp: str
    model_loaded: bool

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FASTAPI APPLICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="HedgeAI ML Service",
    description="Machine Learning Microservice for Risk Prediction & Hedging",
    version=MODEL_VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Logging Middleware
@app.middleware("http")
async def log_requests(request, call_next):
    """Log all incoming requests and outgoing responses"""
    import time
    
    # Log request
    logger.info(f"â†’ {request.method} {request.url.path}")
    logger.debug(f"Request Headers: {dict(request.headers)}")
    
    # Get request body if present
    if request.method in ["POST", "PUT", "PATCH"]:
        body = await request.body()
        if body:
            logger.debug(f"Request Body: {body.decode('utf-8')[:1000]}")  # First 1000 chars
    
    # Process request
    start_time = time.time()
    response = await call_next(request)
    duration = (time.time() - start_time) * 1000
    
    # Log response
    logger.info(f"â† {request.method} {request.url.path} {response.status_code} {duration:.2f}ms")
    logger.debug(f"Response Headers: {dict(response.headers)}")
    
    return response

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLOBAL MODEL INSTANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MLModel:
    """Singleton ML model manager"""
    _instance = None
    _model = None
    _model_metadata = {}

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
            cls._instance.load_model()
        return cls._instance

    def load_model(self):
        """Load ML model from disk"""
        try:
            model_path = Path(MODEL_PATH)
            
            # Check if model file exists
            if not model_path.exists():
                logger.warning(f"Model file not found at {MODEL_PATH}. Using mock predictions.")
                self._model = None
                self._model_metadata = {
                    "name": "PPO_v3_Mock",
                    "version": MODEL_VERSION,
                    "trained_at": "2025-01-15T10:30:00Z",
                    "sharpe_ratio": 1.72,
                    "max_drawdown": -0.074,
                    "win_rate": 0.682
                }
                return

            # Load actual model
            self._model = joblib.load(model_path)
            self._model_metadata = {
                "name": f"{MODEL_TYPE}_Agent",
                "version": MODEL_VERSION,
                "trained_at": datetime.now().isoformat(),
                "sharpe_ratio": 1.72,
                "max_drawdown": -0.074,
                "win_rate": 0.682
            }
            logger.info(f"âœ… Model loaded successfully from {MODEL_PATH}")
            
        except Exception as e:
            logger.error(f"âŒ Error loading model: {str(e)}")
            self._model = None
            raise

    def predict(self, portfolio_data: PortfolioData) -> Dict[str, Any]:
        """Generate risk prediction"""
        
        # Extract features from portfolio
        total_value = portfolio_data.totalValue
        num_positions = len(portfolio_data.positions)
        
        # Calculate portfolio Greeks
        total_delta = sum(p.delta or 0 for p in portfolio_data.positions)
        total_gamma = sum(p.gamma or 0 for p in portfolio_data.positions)
        total_vega = sum(p.vega or 0 for p in portfolio_data.positions)
        
        # Calculate concentration (largest position / total value)
        max_position_value = max(p.quantity * p.price for p in portfolio_data.positions)
        concentration = max_position_value / total_value if total_value > 0 else 0
        
        # If actual model exists, use it
        if self._model is not None:
            try:
                # Prepare features (customize based on your model)
                features = np.array([[
                    total_value,
                    num_positions,
                    total_delta,
                    total_gamma,
                    total_vega,
                    concentration
                ]])
                
                # Get prediction from actual model
                # prediction = self._model.predict(features)
                # For now, fall through to mock
                pass
            except Exception as e:
                logger.error(f"Model prediction error: {str(e)}")

        # Mock prediction logic (replace with actual model inference)
        # Risk score based on concentration and Greeks exposure
        risk_score = min(100, int(
            50 +  # Base risk
            (concentration * 30) +  # Concentration risk
            (abs(total_delta / total_value) * 20 if total_value > 0 else 0)  # Delta risk
        ))
        
        # Volatility estimation
        volatility = 0.15 + (concentration * 0.1) + np.random.uniform(-0.02, 0.02)
        
        # VaR estimation (95% and 99% confidence)
        z_95 = 1.645
        z_99 = 2.326
        var_95 = -total_value * volatility * z_95 * np.sqrt(1/252)  # Daily VaR
        var_99 = -total_value * volatility * z_99 * np.sqrt(1/252)
        
        # Sharpe ratio estimation
        sharpe_ratio = max(0, 1.8 - (risk_score / 100) * 1.0)
        
        # Generate recommendation
        if risk_score > 80:
            recommendation = "CRITICAL: Immediate hedging required. Portfolio exposure is dangerously high."
        elif risk_score > 65:
            recommendation = "WARNING: Consider reducing delta exposure through delta-neutral hedging."
        elif risk_score > 45:
            recommendation = "MODERATE: Portfolio risk is within acceptable range. Monitor closely."
        else:
            recommendation = "LOW: Portfolio is well-hedged. No immediate action required."
        
        # Confidence based on data quality
        confidence = 0.85 if all(p.delta is not None for p in portfolio_data.positions) else 0.65
        
        return {
            "riskScore": risk_score,
            "volatility": round(volatility, 4),
            "var95": round(var_95, 2),
            "var99": round(var_99, 2),
            "sharpeRatio": round(sharpe_ratio, 4),
            "recommendation": recommendation,
            "confidence": round(confidence, 2),
            "timestamp": datetime.now().isoformat()
        }

    def get_metadata(self) -> Dict[str, Any]:
        """Get model metadata"""
        return self._model_metadata


# Initialize model singleton
ml_model = MLModel.get_instance()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "model_loaded": ml_model._model is not None
    }

@app.get("/model-info", response_model=ModelInfo)
async def get_model_info():
    """Get ML model information"""
    metadata = ml_model.get_metadata()
    return {
        "name": metadata["name"],
        "version": metadata["version"],
        "trained_at": metadata["trained_at"],
        "performance_metrics": {
            "sharpe_ratio": metadata["sharpe_ratio"],
            "max_drawdown": metadata["max_drawdown"],
            "win_rate": metadata["win_rate"]
        }
    }

@app.post("/predict-risk", response_model=MLPredictionResponse)
async def predict_risk(request: MLPredictionRequest):
    """
    Predict risk metrics for a given portfolio
    
    Returns risk score, volatility, VaR, and recommendation
    """
    try:
        logger.info(f"Prediction request for portfolio: {request.portfolioId}")
        logger.debug(f"Portfolio Data: {request.portfolioData.dict()}")
        
        # Get prediction
        prediction = ml_model.predict(request.portfolioData)
        
        logger.info(f"Prediction completed: Risk Score = {prediction['riskScore']}")
        logger.debug(f"Full Prediction: {prediction}")
        return prediction
        
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Prediction failed: {str(e)}"
        )

@app.post("/recommend-hedge", response_model=HedgingRecommendation)
async def recommend_hedge(portfolio_data: Dict[str, Any]):
    """
    Generate hedging recommendation for portfolio
    """
    try:
        total_delta = portfolio_data.get("delta", 0)
        total_value = portfolio_data.get("totalValue", 0)
        
        # Simple delta hedging logic (customize based on your strategy)
        if abs(total_delta) > 0.15 * total_value:
            contracts_needed = int(abs(total_delta) / 100)  # Assuming 100 delta per contract
            action = "SELL" if total_delta > 0 else "BUY"
            strategy = "Delta Hedging"
            expected_reduction = min(0.8, abs(total_delta) / total_value)
        else:
            contracts_needed = 0
            action = "HOLD"
            strategy = "No Action Required"
            expected_reduction = 0.0
        
        return {
            "action": action,
            "contracts": contracts_needed,
            "strategy": strategy,
            "expectedReduction": expected_reduction
        }
        
    except Exception as e:
        logger.error(f"Hedging recommendation error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Recommendation failed: {str(e)}"
        )

@app.post("/batch-predict", response_model=List[MLPredictionResponse])
async def batch_predict(portfolios: Dict[str, List[MLPredictionRequest]]):
    """
    Batch prediction for multiple portfolios
    """
    try:
        portfolio_list = portfolios.get("portfolios", [])
        predictions = []
        
        for request in portfolio_list:
            prediction = ml_model.predict(request.portfolioData)
            predictions.append(prediction)
        
        logger.info(f"Batch prediction completed for {len(predictions)} portfolios")
        return predictions
        
    except Exception as e:
        logger.error(f"Batch prediction error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch prediction failed: {str(e)}"
        )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STARTUP & SHUTDOWN EVENTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.on_event("startup")
async def startup_event():
    """Actions to perform on startup"""
    logger.info("="*60)
    logger.info("ğŸš€ HedgeAI ML Service Starting...")
    logger.info(f"   Model Type: {MODEL_TYPE}")
    logger.info(f"   Model Version: {MODEL_VERSION}")
    logger.info(f"   Model Path: {MODEL_PATH}")
    logger.info("="*60)

@app.on_event("shutdown")
async def shutdown_event():
    """Actions to perform on shutdown"""
    logger.info("Shutting down ML service...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    reload = os.getenv("RELOAD", "false").lower() == "true"
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=reload,
        log_level="info"
    )
